{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGP for regression\n",
    "\n",
    "Here we'll show the DGP for regression, using small to medium data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/m/home/home1/18/gadichs1/unix/.conda/envs/deepgps_2_2/lib/python3.5/site-packages/spyder/utils/ipython', '/l/gadichs1/gitrepos/aalto/Doubly-Stochastic-DGP', '/u/18/gadichs1/unix/.conda/envs/deepgps_2_2/lib/python35.zip', '/u/18/gadichs1/unix/.conda/envs/deepgps_2_2/lib/python3.5', '/u/18/gadichs1/unix/.conda/envs/deepgps_2_2/lib/python3.5/plat-linux', '/u/18/gadichs1/unix/.conda/envs/deepgps_2_2/lib/python3.5/lib-dynload', '/u/18/gadichs1/unix/.conda/envs/deepgps_2_2/lib/python3.5/site-packages', '/u/18/gadichs1/unix/.conda/envs/deepgps_2_2/lib/python3.5/site-packages/IPython/extensions', '/m/home/home1/18/gadichs1/unix/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from gpflow.likelihoods import Gaussian\n",
    "from gpflow.kernels import RBF, White\n",
    "from gpflow.mean_functions import Constant\n",
    "from gpflow.models.sgpr import SGPR, GPRFITC\n",
    "from gpflow.models.svgp import SVGP\n",
    "from gpflow.models.gpr import GPR\n",
    "from gpflow.training import AdamOptimizer, ScipyOptimizer\n",
    "\n",
    "from scipy.cluster.vq import kmeans2\n",
    "from scipy.stats import norm\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "import sys\n",
    "#sys.path.append(\"/l/gadichs1/gitrepos/aalto/Doubly-Stochastic-DGP/\")\n",
    "print(sys.path)\n",
    "\n",
    "from doubly_stochastic_dgp.dgp import DGP\n",
    "from datasets import Datasets\n",
    "datasets = Datasets(data_path='data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the kin8nm data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 7372, D: 8, Ns: 820\n"
     ]
    }
   ],
   "source": [
    "data = datasets.all_datasets['kin8nm'].get_data()\n",
    "X, Y, Xs, Ys, Y_std = [data[_] for _ in ['X', 'Y', 'Xs', 'Ys', 'Y_std']]\n",
    "print('N: {}, D: {}, Ns: {}'.format(X.shape[0], X.shape[1], Xs.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single layer models\n",
    "\n",
    "Our baseline model is a sparse GP, but since the dataset is small we can also train without minibatches so we'll also compare to a collapsed sparse GP (with analytically optimal $q(\\mathbf u)$) which is known as SGPR in GPflow terminology, and we'll also cpmpare to FITC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_single_layer_models(X, Y, Z):\n",
    "    D = X.shape[1]\n",
    "    m_sgpr = SGPR(X, Y, RBF(D), Z.copy())\n",
    "    m_svgp = SVGP(X, Y, RBF(D), Gaussian(), Z.copy())\n",
    "    m_fitc = GPRFITC(X, Y, RBF(D), Z.copy())\n",
    "    for m in m_sgpr, m_svgp, m_fitc:\n",
    "        m.likelihood.variance = 0.01\n",
    "    return m_sgpr, m_svgp, m_fitc\n",
    "\n",
    "Z_100 = kmeans2(X, 100, minit='points')[0]\n",
    "Z_500 = kmeans2(X, 500, minit='points')[0]\n",
    "m_sgpr, m_svgp, m_fitc = make_single_layer_models(X, Y, Z_100)\n",
    "m_sgpr_500, m_svgp_500, m_fitc_500 = make_single_layer_models(X, Y, Z_500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGP models\n",
    "\n",
    "We'll include a DGP with a single layer here for comparision. We've used a largish minibatch size of $\\text{min}(1000, N)$, but it works fine for smaller batches too\n",
    "\n",
    "In the paper we used 1 sample. Here we'll go up to 10 in celebration of the new implementation (which is much more efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dgp(X, Y, Z, L):\n",
    "    D = X.shape[1]\n",
    "    \n",
    "    # the layer shapes are defined by the kernel dims, so here all \n",
    "    # hidden layers are D dimensional \n",
    "    kernels = []\n",
    "    for l in range(L):\n",
    "        kernels.append(RBF(D))\n",
    "        \n",
    "    # between layer noise (doesn't actually make much difference \n",
    "    # but we include it anyway)\n",
    "    for kernel in kernels[:-1]:\n",
    "        kernel += White(D, variance=1e-5) \n",
    "        \n",
    "    mb = 1000 if X.shape[0] > 1000 else None \n",
    "    model = DGP(X, Y, Z, kernels, Gaussian(), num_samples=10, \\\n",
    "                minibatch_size=mb)\n",
    "\n",
    "    # start the inner layers almost deterministically \n",
    "    for layer in model.layers[:-1]:\n",
    "        layer.q_sqrt = layer.q_sqrt.value * 1e-5\n",
    "    \n",
    "    return model\n",
    "\n",
    "m_dgp1 = make_dgp(X, Y, Z_100, 1)\n",
    "m_dgp2 = make_dgp(X, Y, Z_100, 2)\n",
    "m_dgp3 = make_dgp(X, Y, Z_100, 3)\n",
    "m_dgp4 = make_dgp(X, Y, Z_100, 4)\n",
    "m_dgp5 = make_dgp(X, Y, Z_100, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "We'll calculate test rmse and likelihood in batches (so the larger datasets don't cause memory problems)\n",
    "\n",
    "For the DGP models we need to take an average over the samples for the rmse. The `predict_density` function already does this internally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_assess(model, assess_model, X, Y):\n",
    "    n_batches = max(int(X.shape[0]/1000.), 1)\n",
    "    lik, sq_diff = [], []\n",
    "    for X_batch, Y_batch in zip(np.array_split(X, n_batches),\\\n",
    "                                np.array_split(Y, n_batches)):\n",
    "        l, sq = assess_model(model, X_batch, Y_batch)\n",
    "        lik.append(l)\n",
    "        sq_diff.append(sq)\n",
    "    lik = np.concatenate(lik, 0)\n",
    "    sq_diff = np.array(np.concatenate(sq_diff, 0), dtype=float)\n",
    "    return np.average(lik), np.average(sq_diff)**0.5\n",
    "\n",
    "def assess_single_layer(model, X_batch, Y_batch):\n",
    "    m, v = model.predict_y(X_batch)\n",
    "    lik = np.sum(norm.logpdf(Y_batch*Y_std, loc=m*Y_std,\\\n",
    "                             scale=Y_std*v**0.5),  1)\n",
    "    sq_diff = Y_std**2*((m - Y_batch)**2)\n",
    "    return lik, sq_diff \n",
    "\n",
    "S = 100\n",
    "def assess_sampled(model, X_batch, Y_batch):\n",
    "    m, v = model.predict_y(X_batch, S)\n",
    "    S_lik = np.sum(norm.logpdf(Y_batch*Y_std, loc=m*Y_std,\\\n",
    "                               scale=Y_std*v**0.5), 2)\n",
    "    lik = logsumexp(S_lik, 0, b=1/float(S))\n",
    "    \n",
    "    mean = np.average(m, 0)\n",
    "    sq_diff = Y_std**2*((mean - Y_batch)**2)\n",
    "    return lik, sq_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "We'll optimize single layer models and using LFBGS and the dgp models with Adam. It will be interesting to compare the result of `m_svgp` compared to `m_dgp1`: if there is a difference it will be down to the optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "  Objective function value: 4570.308663\n",
      "  Number of iterations: 2250\n",
      "  Number of functions evaluations: 2321\n",
      "col sgp           lik: 0.9748, rmse: 0.0866\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS EXCEEDS LIMIT'\n",
      "  Objective function value: 4576.308483\n",
      "  Number of iterations: 5001\n",
      "  Number of functions evaluations: 5212\n",
      "sgp               lik: 0.9749, rmse: 0.0866\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS EXCEEDS LIMIT'\n",
      "  Objective function value: 2080.966339\n",
      "  Number of iterations: 5001\n",
      "  Number of functions evaluations: 5298\n",
      "fitc              lik: 1.1254, rmse: 0.0832\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS EXCEEDS LIMIT'\n",
      "  Objective function value: 2947.244554\n",
      "  Number of iterations: 5001\n",
      "  Number of functions evaluations: 5562\n",
      "col sgp 500       lik: 1.1491, rmse: 0.0763\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS EXCEEDS LIMIT'\n",
      "  Objective function value: 2973.429457\n",
      "  Number of iterations: 5001\n",
      "  Number of functions evaluations: 5240\n",
      "sgp 500           lik: 1.1458, rmse: 0.0763\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS EXCEEDS LIMIT'\n",
      "  Objective function value: 353.986664\n",
      "  Number of iterations: 5001\n",
      "  Number of functions evaluations: 5228\n",
      "fitc 500          lik: 1.2395, rmse: 0.0747\n"
     ]
    }
   ],
   "source": [
    "single_layer_models = [m_sgpr, m_svgp, m_fitc, m_sgpr_500, m_svgp_500, m_fitc_500]\n",
    "single_layer_names = ['col sgp', 'sgp', 'fitc', 'col sgp 500', 'sgp 500', 'fitc 500']\n",
    "\n",
    "s = '{:<16}  lik: {:.4f}, rmse: {:.4f}'\n",
    "\n",
    "for m, name in zip(single_layer_models, single_layer_names):\n",
    "    ScipyOptimizer().minimize(m, maxiter=5000)\n",
    "    lik, rmse = batch_assess(m, assess_single_layer, Xs, Ys)\n",
    "    print(s.format(name, lik, rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the DGP models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dgp1 (sgp+adam)   lik: 0.9292, rmse: 0.0913\n",
      "dgp2              lik: 1.2908, rmse: 0.0664\n",
      "dgp3              lik: 1.3186, rmse: 0.0647\n",
      "dgp4              lik: 1.3177, rmse: 0.0649\n",
      "dgp5              lik: 1.3220, rmse: 0.0645\n"
     ]
    }
   ],
   "source": [
    "for m, name in zip([m_dgp1, m_dgp2, m_dgp3, m_dgp4, m_dgp5],\\\n",
    "                ['dgp1 (sgp+adam)', 'dgp2', 'dgp3', 'dgp4', 'dgp5']):\n",
    "    AdamOptimizer(0.01).minimize(m, maxiter=5000)\n",
    "    lik, rmse = batch_assess(m, assess_sampled, Xs, Ys)\n",
    "    print(s.format(name, lik, rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
