{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import chdir\n",
    "chdir(\"/l/hegdep1/onoffgp/pymodels/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "from scipy.cluster.vq import kmeans\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from numpy.random import RandomState\n",
    "rng = RandomState(1231)\n",
    "%matplotlib inline\n",
    "\n",
    "float_type = tf.float64\n",
    "jitter_level = 1e-5\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "modelPath = \"tfmodels/forestfires/svgp/run01\"\n",
    "tbPath    = \"tfmodels/forestfires/svgp/log/run01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from onofftf.main import Param, DataSet, GaussKL, KernSE, GPConditional\n",
    "from onofftf.utils import modelmanager\n",
    "from gpflow import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"../data/forestfire/forestfires.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lfeatures = ['temp','RH','wind','rain'] #'X','Y','month','day','FFMC','DMC','DC','ISI'\n",
    "ltarget   = ['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_feat = raw_data[lfeatures]\n",
    "raw_target = raw_data[ltarget]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_feat = (raw_feat - raw_feat.mean())/ raw_feat.std()\n",
    "\n",
    "Xraw = norm_feat.values\n",
    "Yraw = raw_target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(413, 4) (413, 1)\n",
      "(104, 4) (104, 1)\n",
      "(413, 4) (413, 1)\n",
      "(104, 4) (104, 1)\n",
      "(414, 4) (414, 1)\n",
      "(103, 4) (103, 1)\n",
      "(414, 4) (414, 1)\n",
      "(103, 4) (103, 1)\n",
      "(414, 4) (414, 1)\n",
      "(103, 4) (103, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold # import KFold\n",
    "kf = KFold(n_splits=5, random_state=1234, shuffle=False)\n",
    "for train_index, test_index in kf.split(Xraw):\n",
    "    Xtrain, Xtest = Xraw[train_index], Xraw[test_index]\n",
    "    Ytrain, Ytest = Yraw[train_index], Yraw[test_index]\n",
    "    print(Xtrain.shape,Ytrain.shape)\n",
    "    print(Xtest.shape,Ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"../data/forestfire/forestfires.csv\")\n",
    "msk = rng.rand(len(raw_data)) < 0.6\n",
    "traindf = raw_data[msk]\n",
    "testdf  = raw_data[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_transform = False\n",
    "\n",
    "lfeatures = ['temp','RH','wind','rain'] #'X','Y','month','day','FFMC','DMC','DC','ISI'\n",
    "ltarget   = ['area']\n",
    "Xtrain = traindf[lfeatures].values\n",
    "Ytrain = traindf[ltarget].values\n",
    "\n",
    "Xtest  = testdf[lfeatures].values\n",
    "Ytest  = testdf[ltarget].values\n",
    "\n",
    "norm_m = Xtrain.mean(axis=0)\n",
    "norm_sd = np.sqrt(Xtrain.var(axis=0))\n",
    "Xtrain = (Xtrain - norm_m) / norm_sd\n",
    "Xtest = (Xtest - norm_m) / norm_sd\n",
    "\n",
    "\n",
    "if log_transform:\n",
    "    train_data = DataSet(Xtrain, np.log(Ytrain+1))\n",
    "else:\n",
    "    train_data = DataSet(Xtrain,Ytrain+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_to_np = lambda _list : [np.array(e) for e in _list]\n",
    "\n",
    "num_iter = 50000\n",
    "num_inducing_f = 40\n",
    "num_minibatch = 100\n",
    "num_data = Xtrain.shape[0]\n",
    "num_features = Xtrain.shape[1]\n",
    "\n",
    "init_fkell = np.ones(num_features)*5\n",
    "init_fkvar = 10.\n",
    "\n",
    "init_noisevar = 0.001\n",
    "\n",
    "q_diag = True\n",
    "include_f_mu = True\n",
    "\n",
    "if include_f_mu: \n",
    "    init_f_mu = 0.\n",
    "\n",
    "init_Zf = kmeans(Xtrain,num_inducing_f)[0]\n",
    "init_u_fm = np.random.randn(num_inducing_f,1)*0.01\n",
    "\n",
    "if q_diag:\n",
    "    init_u_fs_sqrt = np.ones(num_inducing_f).reshape(1,-1).T\n",
    "else:\n",
    "    init_u_fs_sqrt = np.diag(np.ones(num_inducing_f))\n",
    "\n",
    "kern_param_learning_rate = 1e-3\n",
    "indp_param_learning_rate = 1e-3\n",
    "\n",
    "assert(init_Zf.shape[0] == num_inducing_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ****************************************************************\n",
    "# define tensorflow variables and placeholders\n",
    "# ****************************************************************\n",
    "X = tf.placeholder(dtype = float_type)\n",
    "Y = tf.placeholder(dtype = float_type)\n",
    "\n",
    "with tf.name_scope(\"f_kern\"):\n",
    "    fkell = Param(init_fkell,transform=transforms.Log1pe(),\n",
    "                   name=\"lengthscale\",learning_rate = kern_param_learning_rate,summ=True)\n",
    "    fkvar = Param(init_fkvar,transform=transforms.Log1pe(),\n",
    "                   name=\"variance\",learning_rate = kern_param_learning_rate,summ=True)\n",
    "\n",
    "fkern = KernSE(fkell,fkvar)\n",
    "\n",
    "with tf.name_scope(\"likelihood\"):\n",
    "    noisevar = Param(init_noisevar,transform=transforms.Log1pe(),\n",
    "                     name=\"variance\",learning_rate = kern_param_learning_rate,summ=True)\n",
    "\n",
    "with tf.name_scope(\"f_ind\"):\n",
    "    Zf = Param(init_Zf,name=\"z\",learning_rate = indp_param_learning_rate)\n",
    "    u_fm = Param(init_u_fm,name=\"value\",learning_rate = indp_param_learning_rate)\n",
    "    \n",
    "    if include_f_mu:\n",
    "        f_mu = Param(init_f_mu,name=\"fmu\",learning_rate = indp_param_learning_rate,summ=True)\n",
    "    \n",
    "    if q_diag:\n",
    "        u_fs_sqrt = Param(init_u_fs_sqrt,transforms.positive,name=\"variance\",learning_rate = indp_param_learning_rate)\n",
    "    else:\n",
    "        u_fs_sqrt = Param(init_u_fs_sqrt,transforms.LowerTriangular(init_u_fs_sqrt.shape[0]),name=\"variance\",learning_rate = indp_param_learning_rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**define prior kl divergence, variational expectations and predict functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_prior_kl(u_fm,u_fs_sqrt,fkern,Zf):\n",
    "    \n",
    "    kl = GaussKL(u_fm.get_tfv(),u_fs_sqrt.get_tfv(),fkern.K(Zf.get_tfv())) \n",
    "    \n",
    "    return kl\n",
    "\n",
    "def variational_expectations(Y,fmu,fvar,noisevar):\n",
    "    return -0.5 * np.log(2 * np.pi) - 0.5 * tf.log(noisevar) \\\n",
    "            - 0.5 * (tf.square(Y - fmu) + fvar) / noisevar\n",
    "\n",
    "def build_predict(Xnew,u_fm,u_fs_sqrt,fkern,Zf,f_mu=None):\n",
    "    fmean, fvar = GPConditional(Xnew,Zf.get_tfv(), fkern, u_fm.get_tfv(),full_cov=False,\n",
    "                                q_sqrt=u_fs_sqrt.get_tfv(),whiten=False)\n",
    "    \n",
    "    if f_mu is not None:\n",
    "        fmean = fmean + f_mu.get_tfv()\n",
    "        \n",
    "    return fmean, fvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**build model and define lower bound**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get kl term\n",
    "with tf.name_scope(\"kl\"):\n",
    "    kl = build_prior_kl(u_fm,u_fs_sqrt,fkern,Zf)\n",
    "    tf.summary.scalar('kl', kl)\n",
    "\n",
    "with tf.name_scope(\"model_build\"):\n",
    "    if include_f_mu:\n",
    "        fmean,fvar = build_predict(X,u_fm,u_fs_sqrt,fkern,Zf,f_mu)\n",
    "    else:\n",
    "        fmean,fvar = build_predict(X,u_fm,u_fs_sqrt,fkern,Zf)\n",
    "    tf.summary.histogram('fmean',fmean)\n",
    "    tf.summary.histogram('fvar',fvar)\n",
    "\n",
    "# compute likelihood\n",
    "with tf.name_scope(\"var_exp\"):\n",
    "    var_exp = tf.reduce_sum(variational_expectations(Y,fmean,fvar,noisevar.get_tfv()))\n",
    "    tf.summary.scalar('var_exp', var_exp)\n",
    "\n",
    "    # mini-batch scaling\n",
    "    scale =  tf.cast(num_data, float_type) / tf.cast(num_minibatch, float_type)\n",
    "    var_exp_scaled = var_exp * scale\n",
    "    tf.summary.scalar('var_exp_scaled', var_exp_scaled)\n",
    "\n",
    "# final lower bound\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost =  -(var_exp_scaled - kl)\n",
    "    tf.summary.scalar('cost',cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**define optimizer op**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_var_list = tf.trainable_variables()\n",
    "all_lr_list = [var._learning_rate for var in all_var_list]\n",
    "\n",
    "train_opt_group = []\n",
    "\n",
    "for group_learning_rate in set(all_lr_list):\n",
    "    _ind_bool = np.where(np.isin(np.array(all_lr_list),group_learning_rate))[0]\n",
    "    group_var_list = [all_var_list[ind] for ind in _ind_bool]\n",
    "    group_tf_optimizer = tf.train.AdamOptimizer(learning_rate = group_learning_rate)\n",
    "    group_grad_list = tf.gradients(cost,group_var_list)\n",
    "    group_grads_and_vars = list(zip(group_grad_list,group_var_list))\n",
    "\n",
    "\n",
    "    group_train_op = group_tf_optimizer.apply_gradients(group_grads_and_vars)\n",
    "\n",
    "    # Summarize all gradients\n",
    "    for grad, var in group_grads_and_vars:\n",
    "        tf.summary.histogram(var.name + '/gradient', grad)\n",
    "\n",
    "    train_opt_group.append({'names':[var.name for var in group_var_list],\n",
    "                            'vars':group_var_list,\n",
    "                            'learning_rate':group_learning_rate,\n",
    "                            'grads':group_grad_list,\n",
    "                            'train_op':group_train_op})\n",
    "\n",
    "train_op = tf.group(*[group['train_op'] for group in train_opt_group])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# model saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# tensorboard summary\n",
    "summ_merged = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter(tbPath,\n",
    "                                        graph=sess.graph)\n",
    "\n",
    "    \n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('{:>16s}'.format(\"iteration\"),'{:>16s}'.format(\"objective\"),'{:>16s}'.format(\"var_exp\"),'{:>16s}'.format(\"kl\"))\n",
    "\n",
    "for i in range(num_iter):\n",
    "    batch = train_data.next_batch(num_minibatch)\n",
    "    try:    \n",
    "        summary,_ = sess.run([summ_merged,train_op],feed_dict={X : batch[0],Y : batch[1]})\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            _cost    = cost.eval({X : batch[0],Y : batch[1]})\n",
    "            _var_exp = var_exp.eval({X : batch[0],Y :batch[1]})\n",
    "            _kl      = kl.eval({X : batch[0],Y : batch[1]})\n",
    "            print('{:>16d}'.format(i),'{:>16.3f}'.format(_cost),'{:>16.3f}'.format(_var_exp),'{:>16.3f}'.format(_kl))\n",
    "            \n",
    "            if i > 200:\n",
    "                summary_writer.add_summary(summary,i)\n",
    "                summary_writer.flush()\n",
    "\n",
    "       \n",
    "    except KeyboardInterrupt as e:\n",
    "        print(\"Stopping training\")\n",
    "        break\n",
    "        \n",
    "modelmngr = modelmanager(saver, sess, modelPath)\n",
    "modelmngr.save()\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model checking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get test and training predictions\n",
    "def predict_onoff(Xtrain,Xtest):\n",
    "    pred_train = np.maximum(fmean.eval(feed_dict = {X:Xtrain}),0)\n",
    "    pred_test = np.maximum(fmean.eval(feed_dict = {X:Xtest}),0)\n",
    "    if log_transform:\n",
    "        pred_train = np.exp(pred_train)-1\n",
    "        pred_test = np.exp(pred_test)-1\n",
    "        \n",
    "    return pred_train, pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Noise variance   = \" + str(noisevar.get_tfv().eval()))\n",
    "print(\"Kf lengthscales  = \" + str(fkell.get_tfv().eval()))\n",
    "print(\"Kf variance      = \" + str(fkvar.get_tfv().eval()))\n",
    "if include_f_mu:\n",
    "    print(\"f mean           = \" + str(f_mu.get_tfv().eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_train, pred_test = predict_onoff(Xtrain,Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"max pred :\",pred_train.max())\n",
    "print(\"max train:\",Ytrain.max())\n",
    "print(\"max test :\",Ytest.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"train mse:\",np.sqrt(np.mean((pred_train - Ytrain)**2)))\n",
    "print(\"train mae:\",np.mean(np.abs(pred_train - Ytrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"test mse:\",np.sqrt(np.mean((pred_test - Ytest)**2)))\n",
    "print(\"test mae:\",np.mean(np.abs(pred_test - Ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abs_error_svgp_train = np.abs(pred_train - Ytrain)\n",
    "mpl.rcParams['figure.figsize'] = (7,6)\n",
    "plt.hist(abs_error_svgp_train,range=(0,5.),bins=50,alpha=0.5,color = \"#4C997F\",log=False,label=\"svgp\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"error\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.title(\"abosulte error distributions\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
