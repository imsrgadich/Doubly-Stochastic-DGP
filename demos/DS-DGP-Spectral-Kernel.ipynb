{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Gaussian Processes using Spectral Kernels\n",
    "\n",
    "We will be experimenting different number of mixtures of spectral kernels in the doubly stochastic variational inference framework for `deep Gaussian processes`.\n",
    "\n",
    "Comparison of following will be investigated:\n",
    "\n",
    "1) deep GP with gaussian kernel\n",
    "\n",
    "2) deep GP with SM kernel [wilson’13]\n",
    "\n",
    "3) deep GP with GSM kernel [remes’17]\n",
    "\n",
    "We also compare to the standard SVI-GP without deepness (gpflow/SVGP) as a baseline.\n",
    "\n",
    "We already know that Gaussian kernel can be reproduced by a 1-component SM kernel. Hence, the interesting research question is to see how the DGP behaves when we go from gaussian kernel (Q=1) to very spectral kernels (eg Q=10). How does the runtime / accuracy / optimization / kernel behave, do we get overfitting?\n",
    "\n",
    "\n",
    "`The core would then be 5x10=50 experiments with 1..5 layers, and 1..10 spectral components. These will all be run with the deep GP of Salimbeni.`\n",
    "\n",
    "Also, you should run baselines:\n",
    "\n",
    "- [5] deepGP with gaussian kernel with 1..5 layers\n",
    "- [1] SVGP with gaussian kernel\n",
    "- [10] SVGP with spectral kernel with 1..10 components\n",
    "\n",
    "This will in total give us 50+5+1+10=66 experiments.\n",
    "\n",
    "You should use the “double stochastic” paper’s fig1 datasets for this, start from eg. the “power” dataset, and follow the same experimental procedure as Salimbeni.\n",
    "\n",
    "Initialise the inducing locations by k-means, and initialise the inducing values to N(0,0.1). For the spectral kernels you need to do random restarts with different initial spectral components following the strategy of Wilson’13 at\n",
    "\n",
    "https://people.orie.cornell.edu/andrew/code/\n",
    "\n",
    "where check the steps 7+8. However the first spectral component should always be initialised at mu=0. Thus only do the `random restarts` for the q=2..10.\n",
    "\n",
    "Also you need to try different step sizes in the Adam optimiser, while mini batch can probably be fixed to some sensible value (maybe 100 throughout?). Record the trace plots over epochs over both training/test performance. It would be convenient to have only single train/test folds (eg. 70/30) that are fixed in the very beginning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/m/home/home1/18/gadichs1/unix/.conda/envs/deepgps_2_2/lib/python3.5/site-packages/spyder/utils/ipython', '/l/gadichs1/gitrepos/aalto/Doubly-Stochastic-DGP', '/u/18/gadichs1/unix/.conda/envs/deepgps_2_2/lib/python35.zip', '/u/18/gadichs1/unix/.conda/envs/deepgps_2_2/lib/python3.5', '/u/18/gadichs1/unix/.conda/envs/deepgps_2_2/lib/python3.5/plat-linux', '/u/18/gadichs1/unix/.conda/envs/deepgps_2_2/lib/python3.5/lib-dynload', '/u/18/gadichs1/unix/.conda/envs/deepgps_2_2/lib/python3.5/site-packages', '/u/18/gadichs1/unix/.conda/envs/deepgps_2_2/lib/python3.5/site-packages/IPython/extensions', '/m/home/home1/18/gadichs1/unix/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib.use(\"Agg\")\n",
    "%matplotlib inline \n",
    "\n",
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpflow.likelihoods import Gaussian\n",
    "from gpflow.kernels import RBF, White\n",
    "from gpflow.mean_functions import Constant\n",
    "from gpflow.models.sgpr import SGPR, GPRFITC\n",
    "from gpflow.models.svgp import SVGP\n",
    "from gpflow.models.gpr import GPR\n",
    "from gpflow.training import AdamOptimizer, ScipyOptimizer\n",
    "from gpflow.params import Parameter\n",
    "\n",
    "from scipy.cluster.vq import kmeans2\n",
    "from scipy.stats import norm\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "from doubly_stochastic_dgp.dgp import DGP\n",
    "from datasets import Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the `power` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 8611, D: 4, Ns: 957\n"
     ]
    }
   ],
   "source": [
    "datasets = Datasets(data_path='data/')\n",
    "\n",
    "data = datasets.all_datasets['power'].get_data()\n",
    "X, Y, Xs, Ys, Y_std = [data[_] for _ in ['X', 'Y', 'Xs', 'Ys', 'Y_std']]\n",
    "print('N: {}, D: {}, Ns: {}'.format(X.shape[0], X.shape[1], Xs.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Mixture Kernel\n",
    "\n",
    "The *spectral mixture kernel* is the generalization of stationary kernels (or stationary covariance functions). The spectral mixture kernels are the scale-location Gaussian mixture (Wilson, 2013) of the spectral density of a given kernel. Using the principles of Fourier transforms, we can recover the original kernel by simply taking the inverse Fourier transform of the spectral density. The hyperparameters of the spectral mixture kernel can be tuned by optimizing the marginal likelihood but with an additional caution of proper initialization.\n",
    "\n",
    "This kernel representation is statistically powerful as it gives immense flexibility to model spatio-temporal data. Applications have been found in long range crime prediction, time series, image and video extrapolation (Wilson, 2013). This kernel reperesentation also helps us to gain novel intuitions about modelling problems.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the spectral mixture kernel\n",
    "# TODO: do we need slicing here? \n",
    "class SpectralMixtureKernel(gpflow.kernels.Kernel):\n",
    "    def __init__(self, num_mixtures=1,input_dim=1, mixture_weights=[],\\\n",
    "                 mixture_scales=[],mixture_means=[],\\\n",
    "                 variance=1.0, lengthscales=1.0,\\\n",
    "                 active_dims=None,ARD=False,name=None):\n",
    "        '''\n",
    "        - num_mixtures is the number of mixtures; denoted as Q in\n",
    "        Wilson 2013.\n",
    "        - input_dim is the dimension of the input to the kernel.\n",
    "        - mixture_variance is \n",
    "        - mixture_means is the list (or array) of means of the mixtures.\n",
    "        - active_dims is the dimension of the X which needs to be used.\n",
    "        - ARD (don't know whether relevant here) specifies whether the\n",
    "        kernel has one weight_variance per dimension (ARD=True) or a \n",
    "        single weight_variance (ARD=False)\n",
    "        '''\n",
    "        super().__init__(input_dim,variance,lengthscale,active_dims,\\\n",
    "                 ARD,name=name)\n",
    "        # Q(num_of_mixtures)=1 then SM kernel is SE Kernel.\n",
    "        self.num_mixtures = num_mixtures # not a parameter\n",
    "        \n",
    "        # need to put a bound of [-100,100] \n",
    "        self.mixture_weights = mixture_weights\n",
    "        self.mixture_scales = Parameter(mixture_scales,\\\n",
    "                                        transform=transforms.positive)\n",
    "        # need to put a bound of [-100,100] but zeroth component should\n",
    "        # have value '0'\n",
    "        self.mixutre_means = Parameter(mixture_means)\n",
    "        \n",
    "        # parameter or gp.params?\n",
    "   \n",
    "    @params_as_tensors\n",
    "    def K(self, X, X2=None):\n",
    "        if X2 is None:\n",
    "            X2 = X\n",
    "        #dist = self.scaled_euclid_dist(X,X2)\n",
    "        \n",
    "        X1 = tf.transpose(tf.expand_dims(X1,-1),perm=[1,2,0])#D x 1 x N1\n",
    "        X2 = tf.expand_dims(tf.transpose(X2,perm=[1,0]),-1)#D x N2 x 1\n",
    "        \n",
    "        t = tf.subtract(X1,X2)\n",
    "        \n",
    "        \n",
    "        return self.variance * tf.minimum(X, tf.transpose(X2))\n",
    "\n",
    "    def Kdiag(self, X):\n",
    "        return self.variance * tf.reshape(X, (-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK: Implement Spectral kernel  # check the links and code \n",
    "# https://people.orie.cornell.edu/andrew/code/\n",
    "# gpflow kernel implementation http://gpflow.readthedocs.io/en/latest/notebooks/kernels.html\n",
    "\n",
    "# gpytorch spectral mixture kernel\n",
    "# https://github.com/cornellius-gp/gpytorch/blob/master/gpytorch/kernels/kernel.py\n",
    "# https://github.com/cornellius-gp/gpytorch/blob/master/gpytorch/kernels/spectral_mixture_kernel.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline: Take SVGP in 1 layer/node GP with Spectral Kernel as baseline it is common for\n",
    "# all experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments are defined as increasing number of layers.\n",
    "# For each deep architecture we have #Baseline and we test for increasing number of spectral\n",
    "# mixtures (i.e., from Q=1 to Q=10). \n",
    "# Compare runtime / accuracy / optimization / kernel behaviour.\n",
    "\n",
    "\n",
    "# Doubly Stochastic help: \n",
    "#https://github.com/ICL-SML/Doubly-Stochastic-DGP/blob/master/demos/demo_regression_UCI.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
