{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Gaussian Processes using Spectral Kernels\n",
    "\n",
    "We will be experimenting different number of mixtures of spectral kernels in the doubly stochastic variational inference framework for `deep Gaussian processes`.\n",
    "\n",
    "Comparison of following will be investigated:\n",
    "\n",
    "1) deep GP with gaussian kernel\n",
    "\n",
    "2) deep GP with SM kernel [wilson’13]\n",
    "\n",
    "3) deep GP with GSM kernel [remes’17]\n",
    "\n",
    "We also compare to the standard SVI-GP without deepness (gpflow/SVGP) as a baseline.\n",
    "\n",
    "We already know that Gaussian kernel can be reproduced by a 1-component SM kernel. Hence, the interesting research question is to see how the DGP behaves when we go from gaussian kernel (Q=1) to very spectral kernels (eg Q=10). How does the runtime / accuracy / optimization / kernel behave, do we get overfitting?\n",
    "\n",
    "\n",
    "`The core would then be 5x10=50 experiments with 1..5 layers, and 1..10 spectral components. These will all be run with the deep GP of Salimbeni.`\n",
    "\n",
    "Also, you should run baselines:\n",
    "\n",
    "- [5] deepGP with gaussian kernel with 1..5 layers\n",
    "- [1] SVGP with gaussian kernel\n",
    "- [10] SVGP with spectral kernel with 1..10 components\n",
    "\n",
    "This will in total give us 50+5+1+10=66 experiments.\n",
    "\n",
    "You should use the “double stochastic” paper’s fig1 datasets for this, start from eg. the “power” dataset, and follow the same experimental procedure as Salimbeni.\n",
    "\n",
    "Initialise the inducing locations by k-means, and initialise the inducing values to N(0,0.1). For the spectral kernels you need to do random restarts with different initial spectral components following the strategy of Wilson’13 at\n",
    "\n",
    "https://people.orie.cornell.edu/andrew/code/\n",
    "\n",
    "where check the steps 7+8. However the first spectral component should always be initialised at mu=0. Thus only do the `random restarts` for the q=2..10.\n",
    "\n",
    "Also you need to try different step sizes in the Adam optimiser, while mini batch can probably be fixed to some sensible value (maybe 100 throughout?). Record the trace plots over epochs over both training/test performance. It would be convenient to have only single train/test folds (eg. 70/30) that are fixed in the very beginning.\n",
    "\n",
    "**Why go deep?**\n",
    "\n",
    "(Deep Probabilistic Modeling- Niel Lawrence, NIPS 2017)\n",
    "\n",
    "In a single layer GP we would need *generalized spectral mixture kernel* to learn input dependent lengthscales but the having the hierarchical architecture we can achieve this with simpler kernel increasing the interpretability of the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib.use(\"Agg\")\n",
    "%matplotlib inline \n",
    "\n",
    "from gpflow.likelihoods import Gaussian\n",
    "from gpflow.kernels import RBF, White, Kernel\n",
    "from gpflow.mean_functions import Constant\n",
    "from gpflow.models.sgpr import SGPR, GPRFITC\n",
    "from gpflow.models.svgp import SVGP\n",
    "from gpflow.models.gpr import GPR\n",
    "from gpflow.training import AdamOptimizer, ScipyOptimizer\n",
    "from gpflow.params import Parameter\n",
    "from gpflow.decors import params_as_tensors, autoflow\n",
    "\n",
    "from scipy.cluster.vq import kmeans2\n",
    "from scipy.stats import norm\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "from doubly_stochastic_dgp.dgp import DGP\n",
    "from doubly_stochastic_dgp.spectralmixture import SpectralMixture\n",
    "from datasets import Datasets\n",
    "\n",
    "#import sys\n",
    "#print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the `power` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets = Datasets(data_path='data/')\n",
    "\n",
    "data = datasets.all_datasets['power'].get_data()\n",
    "X, Y, Xs, Ys, Y_std = [data[_] for _ in ['X', 'Y', 'Xs', 'Ys', 'Y_std']]\n",
    "#print('N: {}, D: {}, Ns: {}'.format(X.shape[0], X.shape[1], Xs.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Mixture Kernel\n",
    "\n",
    "The *spectral mixture (SM) kernel* is the generalization of stationary kernels (or stationary covariance functions). The spectral mixture kernels are the scale-location Gaussian mixture (Wilson, 2013) of the spectral density of a given kernel. Using the principles of Fourier transforms, we can recover the original kernel by simply taking the inverse Fourier transform of the spectral density. The hyperparameters of the spectral mixture kernel can be tuned by optimizing the marginal likelihood but with an additional caution of proper initialization.\n",
    "\n",
    "This kernel representation is statistically powerful as it gives immense flexibility to model spatio-temporal data. Applications have been found in long range crime prediction, time series, image and video extrapolation (Wilson, 2013). This kernel reperesentation also helps us to gain novel intuitions about modelling problems.  \n",
    "\n",
    "**A short note on initialization of the hyperparameters:**\n",
    "There are three hyperparameters namely the mixture weights, mixture variances and mixture means. The initialization of these parameters is vital as we might get stuck in an non-optimal solution due to the non-convexity of the problem. The initialization of the parameters is a topic of contention and we discuss next the intuition about these parameters and reasons for their initializations.\n",
    "\n",
    "1. Mixture Weights: The weights represent the variance of the signal (target variable) analogous to $\\sigma_{f}^2$ in the RBF kernel. It is evident from $k_{SM}(x,x)$, where $x$ is the input data, which reduces to the sum of the of the weights. The mixture weights are equally initialized using the standard deviation of the target variable divided by the number of the components. For example, if a weight for a particular mixture is high, it means that the particular frequency in the data explains maximum variance.\n",
    "\n",
    "2. Mixture Variances: It is easier to interpret the variances as inverse length-scales. The mixture means represent a particular frequency in the signal, but the mixture variance  represents the range of the mixture before it changes it frequency or in the data space it represents significant change in the function. These parameters are usually initialized by sampling from truncated Normal distribution with std as the range of the data in each (data) dimension.\n",
    "\n",
    "3. Mixture means: These represent the different frequencies in the data. It would be easier to view them as period (1/freuquency). Optimizing this parameter is plagued by multimodality of the marginal likelihood. This parameter is initialized by inverse of the smallest distance between data points in each dimension. If the Nyquist frequency $(f_n)$ is present, we can sample from uniform distribution from $[0,f_n]$.\n",
    "\n",
    "**Experiments on the hyperparameters:**\n",
    "- ~~*Mixture weights:* What happens to the other parameters if weights are thought to be drawn from a distribution? Can we use PSIS to smooth the weights? What effect does it have on the signal? How does this affect other parameters?~~\n",
    "\n",
    "- *Mixture means:* If we decide on the number of mixtures, then one way to initialize the means is by finding the *fast fourier transform (FFT)* of the signal and using the lowest two frequencies jittered with some noise? Lowest frequencies because smoothness assumption kicks in. ~~As we use spectral kernels for learning the non-stationarity in the data, this would be a bad idea.~~\n",
    "\n",
    "- *Number of Mixtures:* Why don't we select the number of mixtures based on the frequencies from the FFT of the signal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK: Implement Spectral kernel  # check the links and code \n",
    "# https://people.orie.cornell.edu/andrew/code/\n",
    "# gpflow kernel implementation http://gpflow.readthedocs.io/en/latest/notebooks/kernels.html\n",
    "\n",
    "# gpytorch spectral mixture kernel\n",
    "# https://github.com/cornellius-gp/gpytorch/blob/master/gpytorch/kernels/kernel.py\n",
    "# https://github.com/cornellius-gp/gpytorch/blob/master/gpytorch/kernels/spectral_mixture_kernel.py\n",
    "\n",
    "# plotting functions for the kernel\n",
    "def plotkernelsample(k, ax, xmin=-3, xmax=3):\n",
    "    xx = np.linspace(xmin, xmax, 100)[:,None]\n",
    "    K = k.compute_K_symm(xx)\n",
    "    ax.plot(xx, np.random.multivariate_normal(np.zeros(100), K, 3).T)\n",
    "    ax.set_title(k.__class__.__name__)\n",
    "\n",
    "def plotkernelfunction(K, ax, xmin=-3, xmax=3, other=0):\n",
    "    xx = np.linspace(xmin, xmax, 100)[:,None]\n",
    "    K = k.compute_K_symm(xx)\n",
    "    ax.plot(xx, k.compute_K(xx, np.zeros((1,1)) + other))\n",
    "    ax.set_title(k.__class__.__name__ + ' k(x, %f)'%other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test the spectral mixture kernel\n",
    "kern = SpectralMixture(num_mixtures=3,input_dim=4)\n",
    "\n",
    "#sess=tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>prior</th>\n",
       "      <th>transform</th>\n",
       "      <th>trainable</th>\n",
       "      <th>shape</th>\n",
       "      <th>fixed_shape</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SpectralMixture/mixture_scales</th>\n",
       "      <td>Parameter</td>\n",
       "      <td>None</td>\n",
       "      <td>+ve</td>\n",
       "      <td>True</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>True</td>\n",
       "      <td>[[1.54514445931844, 3.011511010970395, 0.25073...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SpectralMixture/mixutre_means</th>\n",
       "      <td>Parameter</td>\n",
       "      <td>None</td>\n",
       "      <td>(none)</td>\n",
       "      <td>True</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>True</td>\n",
       "      <td>[[8.409922044506956, 5.707194625543655, 3.3423...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SpectralMixture/mixture_weights</th>\n",
       "      <td>Parameter</td>\n",
       "      <td>None</td>\n",
       "      <td>(none)</td>\n",
       "      <td>True</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.33556560842250027, 0.33556560842250027, 0.3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     class prior transform  trainable   shape  \\\n",
       "SpectralMixture/mixture_scales   Parameter  None       +ve       True  (3, 4)   \n",
       "SpectralMixture/mixutre_means    Parameter  None    (none)       True  (3, 4)   \n",
       "SpectralMixture/mixture_weights  Parameter  None    (none)       True    (3,)   \n",
       "\n",
       "                                 fixed_shape  \\\n",
       "SpectralMixture/mixture_scales          True   \n",
       "SpectralMixture/mixutre_means           True   \n",
       "SpectralMixture/mixture_weights         True   \n",
       "\n",
       "                                                                             value  \n",
       "SpectralMixture/mixture_scales   [[1.54514445931844, 3.011511010970395, 0.25073...  \n",
       "SpectralMixture/mixutre_means    [[8.409922044506956, 5.707194625543655, 3.3423...  \n",
       "SpectralMixture/mixture_weights  [0.33556560842250027, 0.33556560842250027, 0.3...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>prior</th>\n",
       "      <th>transform</th>\n",
       "      <th>trainable</th>\n",
       "      <th>shape</th>\n",
       "      <th>fixed_shape</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SpectralMixture/mixture_scales</th>\n",
       "      <td>Parameter</td>\n",
       "      <td>None</td>\n",
       "      <td>+ve</td>\n",
       "      <td>True</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>True</td>\n",
       "      <td>[[1.54514445931844, 3.011511010970395, 0.25073...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SpectralMixture/mixutre_means</th>\n",
       "      <td>Parameter</td>\n",
       "      <td>None</td>\n",
       "      <td>(none)</td>\n",
       "      <td>True</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>True</td>\n",
       "      <td>[[8.409922044506956, 5.707194625543655, 3.3423...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SpectralMixture/mixture_weights</th>\n",
       "      <td>Parameter</td>\n",
       "      <td>None</td>\n",
       "      <td>(none)</td>\n",
       "      <td>True</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.33556560842250027, 0.33556560842250027, 0.3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     class prior transform  trainable   shape  \\\n",
       "SpectralMixture/mixture_scales   Parameter  None       +ve       True  (3, 4)   \n",
       "SpectralMixture/mixutre_means    Parameter  None    (none)       True  (3, 4)   \n",
       "SpectralMixture/mixture_weights  Parameter  None    (none)       True    (3,)   \n",
       "\n",
       "                                 fixed_shape  \\\n",
       "SpectralMixture/mixture_scales          True   \n",
       "SpectralMixture/mixutre_means           True   \n",
       "SpectralMixture/mixture_weights         True   \n",
       "\n",
       "                                                                             value  \n",
       "SpectralMixture/mixture_scales   [[1.54514445931844, 3.011511010970395, 0.25073...  \n",
       "SpectralMixture/mixutre_means    [[8.409922044506956, 5.707194625543655, 3.3423...  \n",
       "SpectralMixture/mixture_weights  [0.33556560842250027, 0.33556560842250027, 0.3...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Baseline: Take SVGP in 1 layer/node GP with Spectral Kernel as baseline it is common for\n",
    "# all experiments.\n",
    "kern.as_pandas_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments are defined as increasing number of layers.\n",
    "# For each deep architecture we have #Baseline and we test for increasing number of spectral\n",
    "# mixtures (i.e., from Q=1 to Q=10). \n",
    "# Compare runtime / accuracy / optimization / kernel behaviour.\n",
    "\n",
    "\n",
    "# Doubly Stochastic help: \n",
    "#https://github.com/ICL-SML/Doubly-Stochastic-DGP/blob/master/demos/demo_regression_UCI.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = GPflow.ekernels.RBF(Ee, ARD=True)\n",
    "\n",
    "@GPflow.param.AutoFlow((tf.float64,))\n",
    "def eval_K(kernel, input):\n",
    "    return kernel.K(input)\n",
    "\n",
    "k = eval_K(kern, z)\n",
    "\n",
    "@GPflow.param.AutoFlow((tf.float64,),(tf.float64,),(tf.float64,))\n",
    "def eval_exKxz(kernel, z, x, xx):\n",
    "    return kernel.exKxz(z, x, xx)\n",
    "\n",
    "exkxz = eval_exKxz(kern, z, xmu, xcov)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
